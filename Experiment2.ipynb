{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330d8ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import manifold\n",
    "from collections import Counter\n",
    "import re\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AdamW, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd19ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8497e2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a699f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models\n",
    "!mkdir -p cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765eed1b",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Here Ban means either to place user in time out or to permanently hide the user's comments on the channel's current and future live streams. This mixup is due to the fact that these actions are indistinguishable from others with the extracted data from markChatItemsByAuthorAsDeletedAction event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c223c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = pd.read_csv('data/chats_2021-05.csv', na_values='', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8b555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ban = pd.read_csv('data/ban_events.csv', usecols=['channelId', 'originVideoId'],na_values='', keep_default_na=False)\n",
    "delet = pd.read_csv('data/deletion_events.csv',na_values='', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4909b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "delet = delet[delet['retracted'] == 0]\n",
    "delet['deleted'] = True #mark the chat that be deleted\n",
    "chat_df = pd.merge(chats, delet[['id', 'deleted']], how='left')\n",
    "chat_df['deleted'].fillna(False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df23afcd",
   "metadata": {},
   "source": [
    "## Filter out non-Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ac889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpkr = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\uAC00-\\uD7A3]')\n",
    "jp = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46be98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_jpkr = chat_df[chat_df['body'].apply(lambda x: True if jpkr.search(x) else False)]\n",
    "chat_jp = chat_df[chat_df['body'].apply(lambda x: True if jp.search(x) else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66a70e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original length :  75726648\n",
      "korean/japanese length :  45200323\n",
      "only japanese length :  45049772\n"
     ]
    }
   ],
   "source": [
    "print('original length : ', len(chat_df))\n",
    "print('korean/japanese length : ', len(chat_jpkr))\n",
    "print('only japanese length : ', len(chat_jp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0939ba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original deletion:  13120\n",
      "japanese/korean deletion:  2266\n",
      "only japanese deletion:  2247\n"
     ]
    }
   ],
   "source": [
    "print('original deletion: ', len(chat_df[chat_df['deleted'] == True]))\n",
    "print('japanese/korean deletion: ', len(chat_jpkr[chat_jpkr['deleted'] == True]))\n",
    "print('only japanese deletion: ', len(chat_jp[chat_jp['deleted'] == True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b08bd",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c7bd1",
   "metadata": {},
   "source": [
    "### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba9b3537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "      <th>membership</th>\n",
       "      <th>isModerator</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>id</th>\n",
       "      <th>channelId</th>\n",
       "      <th>originVideoId</th>\n",
       "      <th>originChannelId</th>\n",
       "      <th>deleted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-01T00:00:00.112000+00:00</td>\n",
       "      <td>そうじゃないｗ</td>\n",
       "      <td>1 year</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66b9d029a3e93df01b2626a77a5230c71abe5890</td>\n",
       "      <td>606b88eef42cc40a9e055d9af6deaf5e76244c02</td>\n",
       "      <td>S8tYbUIoHM0</td>\n",
       "      <td>UCp-5t9SrOQwXMU7iIjQfARg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-01T00:00:00.141000+00:00</td>\n",
       "      <td>いーやバナナだね</td>\n",
       "      <td>2 months</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63cc28b37c760c52156ad2ea8d3e4036a29b19d9</td>\n",
       "      <td>ed808b843c98965376208c6e7aeb12ee122aa9f1</td>\n",
       "      <td>TfRFrbFbE2k</td>\n",
       "      <td>UChUJbHiTVeGrSkTdBzVfNCQ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-01T00:00:00.172000+00:00</td>\n",
       "      <td>大成功でしょ</td>\n",
       "      <td>non-member</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ff9d8bee4c9608299f7c99a286efd19de1cef0ec</td>\n",
       "      <td>116707993d9886a42c32bc5a1a2b9db2e3524e50</td>\n",
       "      <td>S8tYbUIoHM0</td>\n",
       "      <td>UCp-5t9SrOQwXMU7iIjQfARg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-01T00:00:00.216000+00:00</td>\n",
       "      <td>大成功やろ！</td>\n",
       "      <td>non-member</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>b7b75f473487a932093e1852eafd2e7741c74e28</td>\n",
       "      <td>d8b410fab159d1c7c6aef425312d3d948c26ee8d</td>\n",
       "      <td>S8tYbUIoHM0</td>\n",
       "      <td>UCp-5t9SrOQwXMU7iIjQfARg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-05-01T00:00:00.387000+00:00</td>\n",
       "      <td>寝てもろてｗ</td>\n",
       "      <td>2 years</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>673ce821fb45b6b634b9d513024b361cf2550074</td>\n",
       "      <td>972dd2fed963ba20a7e1e30a09a1314c4730a800</td>\n",
       "      <td>S8tYbUIoHM0</td>\n",
       "      <td>UCp-5t9SrOQwXMU7iIjQfARg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp      body  membership  isModerator  \\\n",
       "0  2021-05-01T00:00:00.112000+00:00   そうじゃないｗ      1 year            0   \n",
       "1  2021-05-01T00:00:00.141000+00:00  いーやバナナだね    2 months            0   \n",
       "3  2021-05-01T00:00:00.172000+00:00    大成功でしょ  non-member            0   \n",
       "4  2021-05-01T00:00:00.216000+00:00    大成功やろ！  non-member            0   \n",
       "7  2021-05-01T00:00:00.387000+00:00    寝てもろてｗ     2 years            0   \n",
       "\n",
       "   isVerified                                        id  \\\n",
       "0           0  66b9d029a3e93df01b2626a77a5230c71abe5890   \n",
       "1           0  63cc28b37c760c52156ad2ea8d3e4036a29b19d9   \n",
       "3           0  ff9d8bee4c9608299f7c99a286efd19de1cef0ec   \n",
       "4           0  b7b75f473487a932093e1852eafd2e7741c74e28   \n",
       "7           0  673ce821fb45b6b634b9d513024b361cf2550074   \n",
       "\n",
       "                                  channelId originVideoId  \\\n",
       "0  606b88eef42cc40a9e055d9af6deaf5e76244c02   S8tYbUIoHM0   \n",
       "1  ed808b843c98965376208c6e7aeb12ee122aa9f1   TfRFrbFbE2k   \n",
       "3  116707993d9886a42c32bc5a1a2b9db2e3524e50   S8tYbUIoHM0   \n",
       "4  d8b410fab159d1c7c6aef425312d3d948c26ee8d   S8tYbUIoHM0   \n",
       "7  972dd2fed963ba20a7e1e30a09a1314c4730a800   S8tYbUIoHM0   \n",
       "\n",
       "            originChannelId  deleted  \n",
       "0  UCp-5t9SrOQwXMU7iIjQfARg    False  \n",
       "1  UChUJbHiTVeGrSkTdBzVfNCQ    False  \n",
       "3  UCp-5t9SrOQwXMU7iIjQfARg    False  \n",
       "4  UCp-5t9SrOQwXMU7iIjQfARg    False  \n",
       "7  UCp-5t9SrOQwXMU7iIjQfARg    False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_jp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d78815e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 45047525, 1: 2247})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 轉化成 1 跟 0 的 label\n",
    "chat_jp['label'] = chat_jp['deleted'].apply(lambda x: 1 if x == True else 0)\n",
    "Counter(chat_jp['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dbcce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247\n"
     ]
    }
   ],
   "source": [
    "print(len(chat_jp[chat_jp['label'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9fd15",
   "metadata": {},
   "source": [
    "### Membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "104440c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "chat_jp[\"membership\"] = labelencoder.fit_transform(chat_jp[\"membership\"])\n",
    "# chat_jp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251a106",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff8b9e2",
   "metadata": {},
   "source": [
    "## 0. Sample Data\n",
    "- Hyper parameter: sample_rate (normal / deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17dc5e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted Sample :  2247\n",
      "Normal Sample :  4494\n"
     ]
    }
   ],
   "source": [
    "del_length = len(chat_jp[chat_jp['deleted']==True])\n",
    "length = len(chat_jp[chat_jp['deleted']==False])\n",
    "sample_rate = 2\n",
    "\n",
    "deleted_sample = chat_jp[chat_jp['deleted']==True]\n",
    "print('Deleted Sample : ', len(deleted_sample))\n",
    "normal_sample = chat_jp[chat_jp['deleted']==False].sample((sample_rate*len(deleted_sample)))\n",
    "print('Normal Sample : ', len(normal_sample))\n",
    "                                                          \n",
    "sample_chats = normal_sample.append(deleted_sample, ignore_index=True)\n",
    "sample_chats = shuffle(sample_chats).reset_index(drop=True)                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6495ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of all data: 6741\n",
      "Num of training data: 5392\n",
      "Deleted/Normal: 1790/3602\n",
      "Num of validation data: 1349\n",
      "Deleted/Normal: 457/892\n"
     ]
    }
   ],
   "source": [
    "split_rate = 0.8\n",
    "print(\"Num of all data: {}\".format(len(sample_chats)))\n",
    "train_data = sample_chats[:int(len(sample_chats)*split_rate)].reset_index(drop=True)\n",
    "valid_data = sample_chats[int(len(sample_chats)*split_rate):].reset_index(drop=True)\n",
    "print(\"Num of training data: {}\".format(len(train_data)))\n",
    "deleted_train = len(train_data[train_data['deleted']==True])\n",
    "normal_train = len(train_data[train_data['deleted']==False])\n",
    "print(\"Deleted/Normal: {}/{}\".format(deleted_train, normal_train))\n",
    "print(\"Num of validation data: {}\".format(len(valid_data)))\n",
    "deleted_valid = len(valid_data[valid_data['deleted']==True])\n",
    "normal_valid = len(valid_data[valid_data['deleted']==False])\n",
    "print(\"Deleted/Normal: {}/{}\".format(deleted_valid, normal_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f923a33",
   "metadata": {},
   "source": [
    "## 1. bert-base-japanese-whole-word-masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e60d63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vtuber_Dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        super(Vtuber_Dataset,self).__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        body = self.df.loc[idx,\"body\"][:512]\n",
    "        target = self.df.loc[idx,\"label\"]\n",
    "\n",
    "            \n",
    "        res = tokenizer(body, return_tensors=\"pt\",padding = \"max_length\", max_length = 512)\n",
    "        input_ids = res[\"input_ids\"].squeeze(0)\n",
    "        att_mask  = res[\"attention_mask\"].squeeze(0)\n",
    "        try:\n",
    "            assert input_ids.shape[0] == 512\n",
    "        except:\n",
    "            print(\"error found\")\n",
    "            body = self.df.loc[idx+1,\"body\"]\n",
    "            target = torch.tensor(self.df.loc[idx+1,\"label\"])\n",
    "\n",
    "\n",
    "            res = tokenizer(body, return_tensors=\"pt\",padding = \"max_length\")\n",
    "            input_ids = res[\"input_ids\"].squeeze(0)\n",
    "            att_mask  = res[\"attention_mask\"].squeeze(0)\n",
    "            \n",
    "        return input_ids,att_mask,target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9348f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vtu_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "        \n",
    "    def forward(self,**inputs):\n",
    "        pool_feature = self.backbone(**inputs)\n",
    "        return pool_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f36ad8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_test(preds, targets, device):\n",
    "    preds = torch.FloatTensor(preds)\n",
    "    targets = torch.FloatTensor(targets)\n",
    "    preds = preds.to(device, dtype = torch.int64)\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    correct_results_sum = 0\n",
    "    correct_results_sum = (preds == targets).sum().float()\n",
    "    \n",
    "    return (correct_results_sum / len(targets))*100, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74958422",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "bert = AutoModel.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "# model =  BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "add3c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    " \n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        outputs = self.bert(input_ids, attention_mask = attention_mask)\n",
    "        \n",
    "        cls_hs = outputs.last_hidden_state[:, -1, :]\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d1c2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Vtuber_Dataset(train_data, tokenizer)\n",
    "valid_dataset = Vtuber_Dataset(valid_data, tokenizer)\n",
    "trainloader = DataLoader(train_dataset, batch_size = 1, shuffle = True)\n",
    "validloader = DataLoader(valid_dataset, batch_size = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dfb30e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_Arch(bert)\n",
    "model = model.to(device)\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 1e-2,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5,eps=1e-8)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "gradient_accumulation_steps = 32\n",
    "total_steps = len(trainloader)// gradient_accumulation_steps * 5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73b333e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, device, optimizer, scheduler, gradient_accumulation_steps):\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    acc_total = 0\n",
    "    logit_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for step, batch in enumerate(trainloader):\n",
    "        input_ids, attention_mask, label = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        label = label.to(device)\n",
    "        model = model.to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        logit = outputs\n",
    "        \n",
    "        \n",
    "#         print(logit)\n",
    "\n",
    "        loss = criterion(logit, label)\n",
    "\n",
    "        loss_total += loss.item()\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step() # update all parameters\n",
    "            scheduler.step() # Update learning rate schedule\n",
    "            optimizer.zero_grad() # initialize the gradient so that it wont repeat accumulate itself(update the params)\n",
    "            model.zero_grad()\n",
    "            # print(\"in step:%s,loss:%s\"%(str(step),str(loss)),end = \"\\r\")\n",
    "            \n",
    "        _, logit = torch.max(logit, dim=1)\n",
    "        \n",
    "        logit = logit.tolist()\n",
    "        label = label.tolist()\n",
    "        logit_list += logit\n",
    "        label_list += label\n",
    "\n",
    "    acc,pred_logit = acc_test(logit_list, label_list, device)\n",
    "        \n",
    "    return acc, loss_total/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe8b2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, validloader, device, scheduler):\n",
    "    loss_total = 0\n",
    "    acc_total = 0\n",
    "    logit_list = []\n",
    "    label_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(validloader):\n",
    "            input_ids, attention_mask, label= batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            label = label.to(device)\n",
    "            model = model.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "#             loss = outputs.loss\n",
    "            logit = outputs\n",
    "    \n",
    "            loss = criterion(logit, label)\n",
    "            loss_total += loss\n",
    "            \n",
    "            _, logit = torch.max(logit, dim=1)\n",
    "\n",
    "            logit = logit.tolist()\n",
    "            label = label.tolist()\n",
    "            logit_list += logit\n",
    "            label_list += label\n",
    "    \n",
    "    acc_valid, pred_logit = acc_test(logit_list, label_list, device)\n",
    "    return acc_valid, loss_total/len(validloader), label_list, logit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b2825b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1|| || train loss:0.5347 || valid loss:0.5009 || train acc:76.7619|| valid acc:80.3558\n",
      "Epoch: 2|| || train loss:0.4724 || valid loss:0.4898 || train acc:83.5497|| valid acc:81.6901\n",
      "Epoch: 3|| || train loss:0.4399 || valid loss:0.4838 || train acc:86.9622|| valid acc:82.2832\n",
      "Epoch: 4|| || train loss:0.4162 || valid loss:0.4825 || train acc:89.5401|| valid acc:82.2832\n",
      "Epoch: 5|| || train loss:0.4001 || valid loss:0.4838 || train acc:91.2834|| valid acc:82.4314\n",
      "Epoch: 6|| || train loss:0.3974 || valid loss:0.4838 || train acc:91.5059|| valid acc:82.4314\n",
      "Epoch: 7|| || train loss:0.3981 || valid loss:0.4838 || train acc:91.5245|| valid acc:82.4314\n",
      "Epoch: 8|| || train loss:0.3976 || valid loss:0.4838 || train acc:91.4874|| valid acc:82.4314\n",
      "Epoch: 9|| || train loss:0.3956 || valid loss:0.4838 || train acc:91.8027|| valid acc:82.4314\n",
      "Epoch:10|| || train loss:0.3974 || valid loss:0.4838 || train acc:91.3947|| valid acc:82.4314\n",
      "Epoch:11|| || train loss:0.3989 || valid loss:0.4838 || train acc:91.4503|| valid acc:82.4314\n",
      "Epoch:12|| || train loss:0.3963 || valid loss:0.4838 || train acc:91.7470|| valid acc:82.4314\n",
      "Epoch:13|| || train loss:0.3972 || valid loss:0.4838 || train acc:91.4688|| valid acc:82.4314\n",
      "Epoch:14|| || train loss:0.3964 || valid loss:0.4838 || train acc:91.6729|| valid acc:82.4314\n",
      "Epoch:15|| || train loss:0.3971 || valid loss:0.4838 || train acc:91.7285|| valid acc:82.4314\n",
      "Epoch:16|| || train loss:0.3975 || valid loss:0.4838 || train acc:91.4503|| valid acc:82.4314\n",
      "Epoch:17|| || train loss:0.3953 || valid loss:0.4838 || train acc:91.8212|| valid acc:82.4314\n",
      "Epoch:18|| || train loss:0.3978 || valid loss:0.4838 || train acc:91.5430|| valid acc:82.4314\n",
      "Epoch:19|| || train loss:0.3978 || valid loss:0.4838 || train acc:91.4132|| valid acc:82.4314\n",
      "Epoch:20|| || train loss:0.3963 || valid loss:0.4838 || train acc:91.7099|| valid acc:82.4314\n"
     ]
    }
   ],
   "source": [
    "loss = 1000000\n",
    "best_loss = 0\n",
    "best_label_list = []\n",
    "best_logit_list = []\n",
    "for epoch in range(10):\n",
    "    train_acc, train_loss= train(model, trainloader, device, optimizer, scheduler, gradient_accumulation_steps)\n",
    "    valid_acc, valid_loss, label_list, logit_list = valid(model, validloader, device, scheduler)\n",
    "    \n",
    "    if valid_loss < loss:\n",
    "        best_loss = valid_loss\n",
    "        best_label_list = label_list\n",
    "        best_logit_list = logit_list\n",
    "    \n",
    "    print(f'Epoch:{epoch+1:2d}||', \n",
    "            f'|| train loss:{train_loss:.4f}',\n",
    "            f'|| valid loss:{valid_loss:.4f}',\n",
    "            f'|| train acc:{train_acc:.4f}'\n",
    "            f'|| valid acc:{valid_acc:.4f}',\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b54fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.88       892\n",
      "           1       0.83      0.60      0.70       457\n",
      "\n",
      "    accuracy                           0.82      1349\n",
      "   macro avg       0.83      0.77      0.79      1349\n",
      "weighted avg       0.83      0.82      0.82      1349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(best_label_list, best_logit_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c32f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a150af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038303d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sharon",
   "language": "python",
   "name": "env_sharon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
